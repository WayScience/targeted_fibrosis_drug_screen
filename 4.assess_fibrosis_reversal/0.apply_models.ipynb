{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply final and shuffled models to the data\n",
    "\n",
    "We will be extracting the healthy and failing probabilities for each single-cell into a parquet file to use for figure generation in the next notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import requests\n",
    "\n",
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "from joblib import load\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../utils\")\n",
    "from training_utils import get_X_y_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set function to download files from GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to download and save a file from a GitHub URL\n",
    "def download_file(github_url: str, save_dir: pathlib.Path) -> None:\n",
    "    \"\"\"Download a file from a GitHub raw URL and load it to a specified directory\n",
    "\n",
    "    Args:\n",
    "        github_url (str): string of a GitHub raw URL of the file to download\n",
    "        save_dir (pathlib.Path): path to directory to save files to\n",
    "    \"\"\"\n",
    "    response = requests.get(github_url)\n",
    "    response.raise_for_status()  # Raise an error for bad responses (4xx, 5xx)\n",
    "\n",
    "    file_name = github_url.split(\"/\")[-1]\n",
    "    file_path = save_dir / file_name\n",
    "    file_path.write_bytes(response.content)\n",
    "\n",
    "    print(f\"File downloaded successfully to {file_path}\")\n",
    "    return file_path.resolve(strict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in the joblib files for the final and shuffled baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded successfully to models/log_reg_fs_plate_4_final_downsample.joblib\n",
      "File downloaded successfully to models/log_reg_fs_plate_4_shuffled_downsample.joblib\n",
      "File downloaded successfully to label_encoder/label_encoder_log_reg_fs_plate_4.joblib\n"
     ]
    }
   ],
   "source": [
    "# Define the GitHub raw file links for final and shuffled models\n",
    "final_model_url = \"https://github.com/WayScience/cellpainting_predicts_cardiac_fibrosis/raw/refs/heads/main/5.machine_learning/models/log_reg_fs_plate_4_final_downsample.joblib\"\n",
    "shuffled_model_url = \"https://github.com/WayScience/cellpainting_predicts_cardiac_fibrosis/raw/refs/heads/main/5.machine_learning/models/log_reg_fs_plate_4_shuffled_downsample.joblib\"\n",
    "label_encoder_url = \"https://github.com/WayScience/cellpainting_predicts_cardiac_fibrosis/raw/refs/heads/main/5.machine_learning/encoder_results/label_encoder_log_reg_fs_plate_4.joblib\"\n",
    "\n",
    "# Create the models directory if it doesn't exist\n",
    "models_dir = pathlib.Path(\"models\")\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Create the models directory if it doesn't exist\n",
    "label_encoder_dir = pathlib.Path(\"label_encoder\")\n",
    "label_encoder_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Download the final and shuffled models\n",
    "final_model_path = download_file(final_model_url, models_dir)\n",
    "shuffled_model_path = download_file(shuffled_model_url, models_dir)\n",
    "encoder_path = download_file(label_encoder_url, label_encoder_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get list of features for models to filter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625\n"
     ]
    }
   ],
   "source": [
    "# Define the GitHub raw link for the feature selected parquet file for the plate used with the models\n",
    "github_url = \"https://github.com/WayScience/cellpainting_predicts_cardiac_fibrosis/raw/refs/heads/main/3.process_cfret_features/data/single_cell_profiles/localhost231120090001_sc_feature_selected.parquet\"\n",
    "\n",
    "# Load the parquet file into memory (not locally)\n",
    "response = requests.get(github_url)\n",
    "response.raise_for_status()  # Ensure the request was successful\n",
    "\n",
    "# Convert the response content into a BytesIO object for pandas to read\n",
    "parquet_file = BytesIO(response.content)\n",
    "\n",
    "# Use pyarrow to read only the schema (column names) from the Parquet file\n",
    "schema = pq.read_schema(parquet_file)\n",
    "\n",
    "# Extract columns that do not start with \"Metadata\"\n",
    "model_features = [col for col in schema.names if not col.startswith(\"Metadata\")]\n",
    "\n",
    "# Output the number of columns to check if it matches correctly\n",
    "print(len(model_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set paths and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded file: ../3.preprocessing_features/data/single_cell_profiles/localhost240928120001_sc_normalized.parquet\n",
      "Loaded file: ../3.preprocessing_features/data/single_cell_profiles/localhost240927060001_sc_normalized.parquet\n",
      "Loaded file: ../3.preprocessing_features/data/single_cell_profiles/localhost240927120001_sc_normalized.parquet\n",
      "Loaded file: ../3.preprocessing_features/data/single_cell_profiles/localhost240926150001_sc_normalized.parquet\n"
     ]
    }
   ],
   "source": [
    "# Directory for probability data to be saved\n",
    "prob_dir = pathlib.Path(\"./prob_data\")\n",
    "prob_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Directory with normalized plate datasets\n",
    "data_dir = pathlib.Path(\"../3.preprocessing_features/data/single_cell_profiles\")\n",
    "\n",
    "# Use rglob to search for files with the suffix *_sc_normalized.parquet\n",
    "parquet_files = list(data_dir.rglob(\"*_sc_normalized.parquet\"))\n",
    "\n",
    "# Load all matching parquet files into a list of DataFrames\n",
    "dfs = [pd.read_parquet(file) for file in parquet_files]\n",
    "\n",
    "# Print the file paths and check the content if needed\n",
    "for file in parquet_files:\n",
    "    print(f\"Loaded file: {file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter the data for the model features and add to dictionary to process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing plate: localhost240928120001\n",
      "Number of unique treatments in localhost240928120001: 47\n",
      "Shape of the model DataFrame: (12740, 644)\n",
      "Processing plate: localhost240927060001\n",
      "Number of unique treatments in localhost240927060001: 46\n",
      "Shape of the model DataFrame: (12391, 644)\n",
      "Processing plate: localhost240927120001\n",
      "Number of unique treatments in localhost240927120001: 47\n",
      "Shape of the model DataFrame: (12901, 644)\n",
      "Processing plate: localhost240926150001\n",
      "Number of unique treatments in localhost240926150001: 46\n",
      "Shape of the model DataFrame: (16564, 644)\n"
     ]
    }
   ],
   "source": [
    "# Dictionary to store plate data\n",
    "plate_data_dict = {}\n",
    "\n",
    "# Loop through and process each parquet file\n",
    "for parquet_file in parquet_files:\n",
    "    # Extract the plate name from the file name\n",
    "    plate_name = parquet_file.name.split(\"_\")[0]\n",
    "    print(f\"Processing plate: {plate_name}\")\n",
    "\n",
    "    # Load the Parquet file\n",
    "    df = pd.read_parquet(parquet_file)\n",
    "\n",
    "    # Drop rows with NaN values in feature columns that the model uses\n",
    "    df = df.dropna(subset=model_features)\n",
    "\n",
    "    # Capitalize the cell type values to match the model\n",
    "    df[\"Metadata_cell_type\"] = df[\"Metadata_cell_type\"].str.capitalize()\n",
    "\n",
    "    # Extract metadata columns\n",
    "    metadata_columns = [col for col in df.columns if col.startswith(\"Metadata_\")]\n",
    "\n",
    "    # Extract feature columns that don't start with \"Metadata_\"\n",
    "    feature_columns = [col for col in df.columns if not col.startswith(\"Metadata_\")]\n",
    "\n",
    "    # Filter columns in the data frame to only include those in the model\n",
    "    filtered_feature_columns = [col for col in feature_columns if col in model_features]\n",
    "\n",
    "    # Filter the DataFrame to keep only the desired columns\n",
    "    model_df = df[metadata_columns + filtered_feature_columns]\n",
    "\n",
    "    # Store the processed DataFrame in the dictionary under the key \"model_df\"\n",
    "    plate_data_dict[plate_name] = {\"model_df\": model_df}\n",
    "\n",
    "    # Print info about the processed DataFrame\n",
    "    print(\n",
    "        f\"Number of unique treatments in {plate_name}: {df['Metadata_treatment'].nunique()}\"\n",
    "    )\n",
    "    print(f\"Shape of the model DataFrame: {model_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract final model predicted probabilities for each treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting shuffled probabilities from localhost240928120001 data...\n",
      "Extracting shuffled probabilities from localhost240927060001 data...\n",
      "Extracting shuffled probabilities from localhost240927120001 data...\n",
      "Extracting shuffled probabilities from localhost240926150001 data...\n",
      "Extracting final probabilities from localhost240928120001 data...\n",
      "Extracting final probabilities from localhost240927060001 data...\n",
      "Extracting final probabilities from localhost240927120001 data...\n",
      "Extracting final probabilities from localhost240926150001 data...\n"
     ]
    }
   ],
   "source": [
    "# Create a list to store probability DataFrames from each loop iteration\n",
    "prob_dfs = []\n",
    "\n",
    "# Loop through each model in the models directory\n",
    "for model_path in models_dir.iterdir():\n",
    "    model_type = model_path.stem.split(\"_\")[5]  # Get the model type\n",
    "\n",
    "    # Process each plate's data from the plate_data_dict\n",
    "    for plate_name, info in plate_data_dict.items():\n",
    "        print(f\"Extracting {model_type} probabilities from {plate_name} data...\")\n",
    "\n",
    "        # Load in model to apply to datasets\n",
    "        model = load(model_path)\n",
    "\n",
    "        # Load in label encoder\n",
    "        le = load(encoder_path)\n",
    "\n",
    "        # Get unique cell types and their corresponding encoded values\n",
    "        unique_labels = le.classes_\n",
    "        encoded_values = le.transform(unique_labels)\n",
    "\n",
    "        # Create a dictionary mapping encoded values to original labels\n",
    "        label_dict = dict(zip(encoded_values, unique_labels))\n",
    "\n",
    "        # Load in the DataFrame associated with the plate\n",
    "        data_df = info[\"model_df\"].reset_index(drop=True)\n",
    "\n",
    "        # Load in X data to get predicted probabilities\n",
    "        X, _ = get_X_y_data(df=data_df, label=\"Metadata_cell_type\")\n",
    "\n",
    "        # Predict class probabilities for morphology feature data\n",
    "        predicted_probs = model.predict_proba(X)\n",
    "\n",
    "        # Storing probabilities in a pandas DataFrame\n",
    "        prob_df = pd.DataFrame(predicted_probs, columns=model.classes_)\n",
    "\n",
    "        # Update column names in prob_df using the dictionary and add suffix \"_probas\"\n",
    "        prob_df.columns = [label_dict[col] + \"_probas\" for col in prob_df.columns]\n",
    "\n",
    "        # Add a new column called predicted_label for each row\n",
    "        prob_df[\"predicted_label\"] = prob_df.apply(\n",
    "            lambda row: row.idxmax()[:-7], axis=1\n",
    "        )\n",
    "\n",
    "        # Select metadata columns from the data\n",
    "        metadata_columns = data_df.filter(like=\"Metadata_\")\n",
    "\n",
    "        # Combine metadata columns with predicted probabilities DataFrame based on index\n",
    "        prob_df = prob_df.join(metadata_columns)\n",
    "\n",
    "        # Add a new column for model_type\n",
    "        prob_df[\"model_type\"] = model_type\n",
    "\n",
    "        # Append the DataFrame to the list instead of combining immediately\n",
    "        prob_dfs.append(prob_df)\n",
    "\n",
    "# Combine all DataFrames from the list into a single DataFrame after the loop\n",
    "combined_prob_df = pd.concat(prob_dfs, ignore_index=True)\n",
    "\n",
    "# Save combined probability data\n",
    "combined_prob_df.to_csv(f\"{prob_dir}/combined_batch_1_predicted_proba.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display counts for correctly predicted cells across the cell types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failing: 2653 correct predictions out of 9830 cells (26.99%)\n",
      "Healthy: 2648 correct predictions out of 3440 cells (76.98%)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metadata_cell_type</th>\n",
       "      <th>correct_count</th>\n",
       "      <th>fail_count</th>\n",
       "      <th>total_count</th>\n",
       "      <th>percentage_correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Failing</td>\n",
       "      <td>2653</td>\n",
       "      <td>7177</td>\n",
       "      <td>9830</td>\n",
       "      <td>26.988810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Healthy</td>\n",
       "      <td>2648</td>\n",
       "      <td>792</td>\n",
       "      <td>3440</td>\n",
       "      <td>76.976744</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Metadata_cell_type  correct_count  fail_count  total_count  \\\n",
       "0            Failing           2653        7177         9830   \n",
       "1            Healthy           2648         792         3440   \n",
       "\n",
       "   percentage_correct  \n",
       "0           26.988810  \n",
       "1           76.976744  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter rows where Metadata_treatment is 'DMSO'\n",
    "dmso_rows = combined_prob_df[combined_prob_df[\"Metadata_treatment\"] == \"DMSO\"]\n",
    "\n",
    "# Calculate counts and percentage of correct predictions for each Metadata_cell_type\n",
    "result_counts = (\n",
    "    dmso_rows.groupby(\"Metadata_cell_type\")\n",
    "    .apply(\n",
    "        lambda x: pd.Series(\n",
    "            {\n",
    "                \"correct_count\": (\n",
    "                    x[\"predicted_label\"] == x[\"Metadata_cell_type\"]\n",
    "                ).sum(),\n",
    "                \"fail_count\": (x[\"predicted_label\"] != x[\"Metadata_cell_type\"]).sum(),\n",
    "                \"total_count\": x.shape[0],\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Calculate the percentage of correct predictions\n",
    "result_counts[\"percentage_correct\"] = (\n",
    "    result_counts[\"correct_count\"] / result_counts[\"total_count\"]\n",
    ") * 100\n",
    "\n",
    "# Print the number of correctly predicted cells with percentage for each Metadata_cell_type\n",
    "for idx, row in result_counts.iterrows():\n",
    "    print(\n",
    "        f\"{row['Metadata_cell_type']}: {row['correct_count']} correct predictions out of \"\n",
    "        f\"{row['total_count']} cells ({row['percentage_correct']:.2f}%)\"\n",
    "    )\n",
    "\n",
    "# Display the results\n",
    "result_counts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fibrosis_machine_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
